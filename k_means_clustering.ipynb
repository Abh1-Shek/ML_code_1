{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pandas as pd\n",
    "DISTANCE_TYPE = 'euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset, k, no_of_iterations):\n",
    "    indices = np.random.choice(len(dataset), k, replace = False)\n",
    "    # print(indices)\n",
    "    # choose the rows corresponding to indices which is randomly selected\n",
    "    centroids = dataset.iloc[indices, :]\n",
    "    # the below line finds the distance between centroids and all the datapoints\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    # print(centroids)\n",
    "    # print(distances)\n",
    "    # structure of distance => [[for one point distace from each centroid], ...]\n",
    "    # the below line assigns each point with the nearest centroid\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    # the main algo \n",
    "    for iteration in range(no_of_iterations):\n",
    "        # below array will store the centroids\n",
    "        centroids = []\n",
    "        # finding the new centroid for each of the k clusters\n",
    "        for cluster in range(k):\n",
    "            temp_centroid = dataset[points == cluster].mean(axis = 0)\n",
    "            centroids.append(temp_centroid)\n",
    "        \n",
    "        # new centroids\n",
    "        centroids = np.vstack(centroids)\n",
    "\n",
    "        distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "        points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    return points, centroids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = pd.read_csv(r'processed_covid_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(dataset, centroids):\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, condition, children, decisions):\n",
    "        self.condition = condition\n",
    "        self.children = children\n",
    "        self.decisions = decisions\n",
    "        \n",
    "class Leaf:\n",
    "    def __init__(self, leaf_value):\n",
    "        self.leaf_value = leaf_value\n",
    "        self.condition = \"THIS IS A LEAF NODE!!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, dataset = None, THRESHOLD_VALUE = 0.0, THRESHOLD_SIZE = 10, IS_CATEGORICAL = [0, 0, 0, 0, 0, 0], TARGET_COLUMN = \"new_cases_classes\", Tree_Type = \"entropy\"):\n",
    "        # this cell contains all the constants please beware\n",
    "        self.THRESHOLD_VALUE = THRESHOLD_VALUE\n",
    "        self.THRESHOLD_SIZE = THRESHOLD_SIZE\n",
    "        self.IS_CATEGORICAL = IS_CATEGORICAL\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "        self.dataset = dataset\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.root = self.build_tree(self.dataset)\n",
    "        \n",
    "    \n",
    "    def set_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    # helper function for getting probability from frequency table\n",
    "    # this function is used in get_entropy()\n",
    "    def get_probability(self, event_info):\n",
    "        SUM = sum(event_info)\n",
    "        for i in range(len(event_info)):\n",
    "                event_info[i] /= SUM\n",
    "        return event_info\n",
    "\n",
    "\n",
    "    # this function gets entropy from frequency table\n",
    "    def get_entropy(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        entropy = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                entropy += p * math.log(1 / p) / math.log(2)\n",
    "        return entropy\n",
    "\n",
    "    # this function gets gini impurity from frequency table\n",
    "    def get_gini_impurity(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        imp = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                imp += p * (1.0 - p)\n",
    "        return imp\n",
    "   \n",
    "    #this funtion sort table by column\n",
    "    def sort_table_by_column(self, table, col):\n",
    "        return table.sort_values(by = [col]).reset_index(drop=True)\n",
    "   \n",
    "    def split_dataset_wrt_column(self, dataset, column_name):\n",
    "        unique_items = dataset[column_name].unique()\n",
    "        tables = []\n",
    "        for item in unique_items:\n",
    "            tables.append(dataset[dataset[column_name] == item])\n",
    "        return tables\n",
    "    def get_count(self, table, target_column, class_name):\n",
    "        # print(class_name)\n",
    "        # ans =  (table[target_column] == class_name).shape[0]\n",
    "        ans = (table[target_column] == class_name).sum()\n",
    "        # print (ans)\n",
    "        return ans\n",
    "    def get_entropy_from_table(self, table, target_column):\n",
    "        unique_classes = table[target_column].unique()\n",
    "        # print(unique_classes)\n",
    "        counts = []\n",
    "        for class_name in unique_classes:\n",
    "            counts.append(self.get_count(table,target_column, class_name))\n",
    "        # print(\"Count is \", counts)\n",
    "        if self.Tree_Type == \"entropy\":\n",
    "            return self.get_entropy(counts)\n",
    "        else:\n",
    "            return self.get_gini_impurity(counts)    # write get_gini_impurity if you want to change the parameter to gini imp from entropy\n",
    "    # this function returns the information gain of the column \"column\" when the target column is \"target_column\" of the table dataset\n",
    "    # only for categorical column or attribute\n",
    "    def get_information_gain(self, dataset, column, target_column):\n",
    "        tables = []\n",
    "        size_table = []\n",
    "        overall_size = dataset.shape[0]\n",
    "        for table in self.split_dataset_wrt_column(dataset, column):\n",
    "            tables.append(table)\n",
    "            size_table.append(table.shape[0])\n",
    "        entropies = []\n",
    "        for table in tables:\n",
    "            # print(table)\n",
    "            entropies.append(self.get_entropy_from_table(table, target_column))\n",
    "        # print(\"entropies=\", entropies)\n",
    "        # entropies = [get_entropy_from_table(table, target_column) for table in tables]\n",
    "        # print(entropies)\n",
    "        entropy_initial = self.get_entropy_from_table(dataset, target_column)    # entropy without splitting\n",
    "        # print(\"entropy_intial=\",entropy_initial)\n",
    "        entropy = sum([(size / overall_size) * entropyi for size, entropyi in zip(size_table, entropies)])  # entropy after splitting\n",
    "        return (entropy_initial - entropy)\n",
    "\n",
    "    def max_index(self, arr):\n",
    "        index = 0\n",
    "        mx = arr[0]\n",
    "        for i in range(len(arr)):\n",
    "            if mx < arr[i]:\n",
    "                index = i\n",
    "                mx = arr[i]\n",
    "        return index\n",
    "    def get_value_with_min_entropy_wrt_continuous_column(self, table, column, target_column):\n",
    "        # step 1: sort the table\n",
    "        new_table = self.sort_table_by_column(table, column)\n",
    "        # print(new_table)\n",
    "        # step 2: get various averages\n",
    "        avg_array = []\n",
    "        length_new_table = len(new_table)\n",
    "        for i in range(length_new_table - 1):\n",
    "            avg_array.append((new_table.at[i,column] + new_table.at[i + 1, column]) / 2)\n",
    "        \n",
    "        # print(avg_array)\n",
    "        # step 3: count before and after averages\n",
    "        IGs = []\n",
    "        parentIG = self.get_entropy_from_table(new_table, target_column)\n",
    "        for i in range(length_new_table - 1):\n",
    "            table1 = new_table.iloc[:i + 1,:]\n",
    "            table2 = new_table.iloc[i + 1:, :]\n",
    "            # print('Table 1')\n",
    "            # print(table1)\n",
    "            # print('Table 2')\n",
    "            # print(table2)\n",
    "            E1 = self.get_entropy_from_table(table1, target_column)\n",
    "            E2 = self.get_entropy_from_table(table2, target_column)\n",
    "            # print('E1=',E1, 'E2=', E2)\n",
    "            E = (len(table1) / len(table)) * E1 + (len(table2) / len(table)) * E2\n",
    "            IG = parentIG - E\n",
    "            IGs.append(IG)\n",
    "        # print(IGs)\n",
    "        # if(len(avg_array) == 0):\n",
    "        #     IGs = [0]\n",
    "        #     avg_array = [1]\n",
    "        index = self.max_index(IGs)\n",
    "        \n",
    "        return avg_array[index], IGs[index]  # split wrt value, IG according to that value\n",
    "        # step 4: calculate the entropy wrt each average\n",
    "        # step 5: determine the best split with most information gain\n",
    "        #This function returns the best column for the split\n",
    "    def get_best_column (self, table, target_column, is_categorical):\n",
    "        values = []\n",
    "        # is_categorical is an array which is true if the data is categorical and false if continuous\n",
    "        IGs = []\n",
    "        for index,column in enumerate(table):\n",
    "            if(column == target_column):\n",
    "                break\n",
    "            # print(column,index)\n",
    "            if(is_categorical[index] == 1):\n",
    "                values.append(None)\n",
    "                IGs.append(self.get_information_gain(table, column, target_column))\n",
    "            else:\n",
    "                #value is the value at which the splitting occurs in the column and IG is the corresponding Info gain\n",
    "                value, IG = self.get_value_with_min_entropy_wrt_continuous_column(table,column,target_column)\n",
    "                values.append(value)\n",
    "                # print(value)\n",
    "                IGs.append(IG)\n",
    "        selected_index = self.max_index(IGs)\n",
    "        if(is_categorical[selected_index]):\n",
    "            return selected_index, table.columns[selected_index], None, IGs[selected_index]\n",
    "        else:\n",
    "            return selected_index, table.columns[selected_index],values[selected_index], IGs[selected_index]\n",
    "    def split_table_wrt_value(self, table,value,column):\n",
    "        table1 = table[table[column] <= value]\n",
    "        table2 = table[table[column] > value]\n",
    "        return table1,table2\n",
    "\n",
    "    # returns the value and probability for the leaf which have maximum probability\n",
    "    def get_value_for_leaf(self, table, target_column):\n",
    "        freq = dict()\n",
    "        unique_classes = table[target_column].unique()\n",
    "        for class_name in unique_classes:\n",
    "            freq[class_name] = 0\n",
    "        for index, row in table.iterrows():\n",
    "            freq[row[target_column]] += 1\n",
    "        mx = 0\n",
    "        value = None\n",
    "        sum_of_freq = 0.0\n",
    "        for class_name, freq_of_class in freq.items():\n",
    "            sum_of_freq += freq_of_class\n",
    "            if mx < freq_of_class:\n",
    "                mx = freq_of_class\n",
    "                value = class_name\n",
    "        return value, mx / sum_of_freq\n",
    "    \n",
    "    def build_tree(self, table):\n",
    "        if len(table) <= self.THRESHOLD_SIZE:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # step1 find the best split\n",
    "        selected_index, column_name, value, best_IG = self.get_best_column(table, self.TARGET_COLUMN, self.IS_CATEGORICAL)\n",
    "        tables = []\n",
    "        decisions = []\n",
    "        condition = None\n",
    "        # categorical value\n",
    "        if value == None:\n",
    "            tables = self.split_dataset_wrt_column(table, column_name)\n",
    "            start_index = 0\n",
    "            for splited_table in tables:\n",
    "                decisions.append(splited_table.iloc[0][column_name])\n",
    "                start_index += len(splited_table)\n",
    "            condition = [None, column_name]\n",
    "        # Continuous Value\n",
    "        else:\n",
    "            table1, table2 = self.split_table_wrt_value(table, value, column_name)\n",
    "            tables.append(table1)\n",
    "            tables.append(table2)\n",
    "            decisions = [None,None]\n",
    "            condition = [value, column_name]\n",
    "        # put some base condition\n",
    "        if best_IG <= self.THRESHOLD_VALUE or len(table) <= self.THRESHOLD_SIZE:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # make tree for each child\n",
    "        children = []\n",
    "        table = table.drop([column_name], axis = 1)\n",
    "        for table in tables:\n",
    "            child = self.build_tree(table)\n",
    "            children.append(child)\n",
    "        # return the current node which is already linked to its children so that current node's parent can link current node\n",
    "        return Node(condition, children,decisions)\n",
    "    \n",
    "    def print_tree(self, root, spacing=\"\"):\n",
    "        if isinstance(root, Leaf):\n",
    "            print(spacing, root.leaf_value)\n",
    "            return\n",
    "        print(spacing, root.condition,root.decisions)\n",
    "        for child in root.children:\n",
    "            self.print_tree(child,spacing+\"--> \")\n",
    "    def print_decision_tree(self):\n",
    "        self.print_tree(self.root)\n",
    "    \n",
    "    def find_index(self, arr,x):\n",
    "        for i in range (len(arr)):\n",
    "            if(arr[i] == x):\n",
    "                return i\n",
    "    def predict_util(self, row,root, starting_index = 0):\n",
    "        #base case\n",
    "        # print(row)\n",
    "        if(isinstance(root,Leaf)):\n",
    "            return root.leaf_value\n",
    "        value_to_check = row.at[starting_index, root.condition[1]]\n",
    "        if root.condition[0] != None:\n",
    "            split_value = root.condition[0]\n",
    "            if value_to_check <= split_value:\n",
    "                child_index = 0\n",
    "            else:\n",
    "                child_index = 1\n",
    "        else:\n",
    "            child_index = self.find_index(root.decisions, value_to_check)\n",
    "        # print(value_to_check, root.decisions)\n",
    "        return self.predict_util(row, root.children[child_index], starting_index)\n",
    "\n",
    "        \n",
    "    def predict_for_table(self, table, root):\n",
    "        predictions = []\n",
    "        for index, row in table.iterrows():\n",
    "            row = row.to_frame().T\n",
    "            # print(\"index =\", index)\n",
    "            # print('predictions =', predict(row, root, index))\n",
    "            predictions.append(self.predict_util(row, root, index))\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.predict_for_table(data, self.root)\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data, THRESHOLD_FOR_ACCURACY = 1):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted)\n",
    "            score += (abs(actual - predicted[0]) <= THRESHOLD_FOR_ACCURACY)\n",
    "\n",
    "        print(score / len(testing_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(indexes, x):\n",
    "    indices = []\n",
    "    for i in indexes:\n",
    "        if i == x:\n",
    "            indices.append(True)\n",
    "        else:\n",
    "            indices.append(False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    def __init__(self, dataset, k, num_of_iterations, TARGET_COLUMN = 'new_cases_classes', Tree_Type = \"entropy\"):\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.k = k\n",
    "        self.num_of_iterations = num_of_iterations\n",
    "        self.indexes, self.centroids = kmeans(dataset.drop([TARGET_COLUMN], axis = 1), k, num_of_iterations)\n",
    "        self.dataset = dataset\n",
    "        self.DTs = self.create_trees()\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "\n",
    "    def prepare_data_util(self, x):\n",
    "        dataset1 = self.dataset[get_indices(self.indexes, x)]\n",
    "        dataset1.reset_index(drop=True, inplace=True)\n",
    "        return dataset1\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets = []\n",
    "        for i in range(self.k):\n",
    "            datasets.append(self.prepare_data_util(i))\n",
    "        return datasets\n",
    "\n",
    "    def create_trees(self):\n",
    "        DTs = []\n",
    "        datasets = self.prepare_data()\n",
    "        # print(len(datasets))\n",
    "        for dataset in datasets:\n",
    "            if self.Tree_Type == \"entropy\":\n",
    "                DTs.append(DecisionTree(dataset))\n",
    "            else:\n",
    "                DTs.append(DecisionTree(dataset, Tree_Type = \"CART\"))\n",
    "        return DTs\n",
    "\n",
    "    def calculate_average(self,  all_predictions, centroids):\n",
    "        average_predicted = []\n",
    "        # print(all_predictions)\n",
    "        row, col = len(all_predictions), len(all_predictions[0])\n",
    "        # print(row,col)\n",
    "        for i in range(col):\n",
    "            sum = 0\n",
    "            average_predicted.append(all_predictions[centroids[i]][i])\n",
    "\n",
    "        return(average_predicted)\n",
    "\n",
    "    def predict(self, testing_data):\n",
    "        all_predictions = []\n",
    "        # print(self.DTs)\n",
    "        for DT in self.DTs:\n",
    "            individual_prediction = DT.predict(testing_data)\n",
    "            all_predictions.append(individual_prediction)\n",
    "        # print(all_predictions)\n",
    "        closest_centroids = find_closest_centroids(testing_data.drop([self.TARGET_COLUMN], axis = 1), self.centroids)\n",
    "        final_predictions = self.calculate_average(all_predictions, closest_centroids)\n",
    "        return final_predictions\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data, THRESHOLD_FOR_ACCURACY = 1):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        # THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted[0])\n",
    "            score += (abs(actual - predicted[0]) <= THRESHOLD_FOR_ACCURACY)\n",
    "\n",
    "        print(score / len(testing_data) * 100)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset5.sample(frac = 0.7)\n",
    "testing_data = dataset5.drop(training_data.index)\n",
    "training_data.reset_index(drop=True, inplace=True)\n",
    "testing_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_ID3A = Forest(training_data, k = 3, num_of_iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.43262411347519\n"
     ]
    }
   ],
   "source": [
    "predictions = forest_ID3A.calculate_accuracy(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_CART = Forest(training_data, k = 3, num_of_iterations = 100, Tree_Type = 'CART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.43262411347519\n"
     ]
    }
   ],
   "source": [
    "predictions = forest_CART.calculate_accuracy(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "52c418bc-bd7d-4061-b6a2-3b59f32782e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
