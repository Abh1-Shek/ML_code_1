{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "DISTANCE_TYPE = 'euclidean'\n",
    "DISTANCE_TYPE2 = 'cityblock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset, k, no_of_iterations):\n",
    "    indices = np.random.choice(len(dataset), k, replace = False)\n",
    "    # print(indices)\n",
    "    # choose the rows corresponding to indices which is randomly selected\n",
    "    centroids = dataset.iloc[indices, :]\n",
    "    # the below line finds the distance between centroids and all the datapoints\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    # print(centroids)\n",
    "    # print(distances)\n",
    "    # structure of distance => [[for one point distace from each centroid], ...]\n",
    "    # the below line assigns each point with the nearest centroid\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    # the main algo \n",
    "    for iteration in range(no_of_iterations):\n",
    "        # below array will store the centroids\n",
    "        centroids = []\n",
    "        # finding the new centroid for each of the k clusters\n",
    "        for cluster in range(k):\n",
    "            temp_centroid = dataset[points == cluster].mean(axis = 0)\n",
    "            centroids.append(temp_centroid)\n",
    "        \n",
    "        # new centroids\n",
    "        centroids = np.vstack(centroids)\n",
    "\n",
    "        distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "        points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    return points, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = pd.read_csv(r'processed_covid_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(dataset, centroids):\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, condition, children, decisions):\n",
    "        self.condition = condition\n",
    "        self.children = children\n",
    "        self.decisions = decisions\n",
    "        \n",
    "class Leaf:\n",
    "    def __init__(self, leaf_value):\n",
    "        self.leaf_value = leaf_value\n",
    "        self.condition = \"THIS IS A LEAF NODE!!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, dataset = None, THRESHOLD_VALUE = 0.0, THRESHOLD_SIZE = 10, IS_CATEGORICAL = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], TARGET_COLUMN = \"new_cases_classes\", Tree_Type = \"entropy\"):\n",
    "        # this cell contains all the constants please beware\n",
    "        self.THRESHOLD_VALUE = THRESHOLD_VALUE\n",
    "        self.THRESHOLD_SIZE = THRESHOLD_SIZE\n",
    "        self.IS_CATEGORICAL = IS_CATEGORICAL\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "        self.dataset = dataset\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.root = self.build_tree(self.dataset)\n",
    "        \n",
    "    \n",
    "    def set_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    # helper function for getting probability from frequency table\n",
    "    # this function is used in get_entropy()\n",
    "    def get_probability(self, event_info):\n",
    "        SUM = sum(event_info)\n",
    "        for i in range(len(event_info)):\n",
    "                event_info[i] /= SUM\n",
    "        return event_info\n",
    "\n",
    "\n",
    "    # this function gets entropy from frequency table\n",
    "    def get_entropy(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        entropy = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                entropy += p * math.log(1 / p) / math.log(2)\n",
    "        return entropy\n",
    "\n",
    "    # this function gets gini impurity from frequency table\n",
    "    def get_gini_impurity(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        imp = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                imp += p * (1.0 - p)\n",
    "        return imp\n",
    "   \n",
    "    #this funtion sort table by column\n",
    "    def sort_table_by_column(self, table, col):\n",
    "        return table.sort_values(by = [col]).reset_index(drop=True)\n",
    "   \n",
    "    def split_dataset_wrt_column(self, dataset, column_name):\n",
    "        unique_items = dataset[column_name].unique()\n",
    "        tables = []\n",
    "        for item in unique_items:\n",
    "            tables.append(dataset[dataset[column_name] == item])\n",
    "        return tables\n",
    "    def get_count(self, table, target_column, class_name):\n",
    "        # print(class_name)\n",
    "        # ans =  (table[target_column] == class_name).shape[0]\n",
    "        ans = (table[target_column] == class_name).sum()\n",
    "        # print (ans)\n",
    "        return ans\n",
    "    def get_entropy_from_table(self, table, target_column):\n",
    "        unique_classes = table[target_column].unique()\n",
    "        # print(unique_classes)\n",
    "        counts = []\n",
    "        for class_name in unique_classes:\n",
    "            counts.append(self.get_count(table,target_column, class_name))\n",
    "        # print(\"Count is \", counts)\n",
    "        if self.Tree_Type == \"entropy\":\n",
    "            return self.get_entropy(counts)\n",
    "        else:\n",
    "            return self.get_gini_impurity(counts)    # write get_gini_impurity if you want to change the parameter to gini imp from entropy\n",
    "    # this function returns the information gain of the column \"column\" when the target column is \"target_column\" of the table dataset\n",
    "    # only for categorical column or attribute\n",
    "    def get_information_gain(self, dataset, column, target_column):\n",
    "        tables = []\n",
    "        size_table = []\n",
    "        overall_size = dataset.shape[0]\n",
    "        for table in self.split_dataset_wrt_column(dataset, column):\n",
    "            tables.append(table)\n",
    "            size_table.append(table.shape[0])\n",
    "        entropies = []\n",
    "        for table in tables:\n",
    "            # print(table)\n",
    "            entropies.append(self.get_entropy_from_table(table, target_column))\n",
    "        # print(\"entropies=\", entropies)\n",
    "        # entropies = [get_entropy_from_table(table, target_column) for table in tables]\n",
    "        # print(entropies)\n",
    "        entropy_initial = self.get_entropy_from_table(dataset, target_column)    # entropy without splitting\n",
    "        # print(\"entropy_intial=\",entropy_initial)\n",
    "        entropy = sum([(size / overall_size) * entropyi for size, entropyi in zip(size_table, entropies)])  # entropy after splitting\n",
    "        return (entropy_initial - entropy)\n",
    "\n",
    "    def max_index(self, arr):\n",
    "        index = 0\n",
    "        mx = arr[0]\n",
    "        for i in range(len(arr)):\n",
    "            if mx < arr[i]:\n",
    "                index = i\n",
    "                mx = arr[i]\n",
    "        return index\n",
    "    def get_value_with_min_entropy_wrt_continuous_column(self, table, column, target_column):\n",
    "        # step 1: sort the table\n",
    "        new_table = self.sort_table_by_column(table, column)\n",
    "        # print(new_table)\n",
    "        # step 2: get various averages\n",
    "        avg_array = []\n",
    "        length_new_table = len(new_table)\n",
    "        for i in range(length_new_table - 1):\n",
    "            avg_array.append((new_table.at[i,column] + new_table.at[i + 1, column]) / 2)\n",
    "        \n",
    "        # print(avg_array)\n",
    "        # step 3: count before and after averages\n",
    "        IGs = []\n",
    "        parentIG = self.get_entropy_from_table(new_table, target_column)\n",
    "        for i in range(length_new_table - 1):\n",
    "            table1 = new_table.iloc[:i + 1,:]\n",
    "            table2 = new_table.iloc[i + 1:, :]\n",
    "            # print('Table 1')\n",
    "            # print(table1)\n",
    "            # print('Table 2')\n",
    "            # print(table2)\n",
    "            E1 = self.get_entropy_from_table(table1, target_column)\n",
    "            E2 = self.get_entropy_from_table(table2, target_column)\n",
    "            # print('E1=',E1, 'E2=', E2)\n",
    "            E = (len(table1) / len(table)) * E1 + (len(table2) / len(table)) * E2\n",
    "            IG = parentIG - E\n",
    "            IGs.append(IG)\n",
    "        # print(IGs)\n",
    "        # if(len(avg_array) == 0):\n",
    "        #     IGs = [0]\n",
    "        #     avg_array = [1]\n",
    "        index = self.max_index(IGs)\n",
    "        \n",
    "        return avg_array[index], IGs[index]  # split wrt value, IG according to that value\n",
    "        # step 4: calculate the entropy wrt each average\n",
    "        # step 5: determine the best split with most information gain\n",
    "        #This function returns the best column for the split\n",
    "    def get_best_column (self, table, target_column, is_categorical):\n",
    "        values = []\n",
    "        # is_categorical is an array which is true if the data is categorical and false if continuous\n",
    "        IGs = []\n",
    "        for index,column in enumerate(table):\n",
    "            if(column == target_column):\n",
    "                break\n",
    "            # print(column,index)\n",
    "            if(is_categorical[index] == 1):\n",
    "                values.append(None)\n",
    "                IGs.append(self.get_information_gain(table, column, target_column))\n",
    "            else:\n",
    "                #value is the value at which the splitting occurs in the column and IG is the corresponding Info gain\n",
    "                value, IG = self.get_value_with_min_entropy_wrt_continuous_column(table,column,target_column)\n",
    "                values.append(value)\n",
    "                # print(value)\n",
    "                IGs.append(IG)\n",
    "        selected_index = self.max_index(IGs)\n",
    "        if(is_categorical[selected_index]):\n",
    "            return selected_index, table.columns[selected_index], None, IGs[selected_index]\n",
    "        else:\n",
    "            return selected_index, table.columns[selected_index],values[selected_index], IGs[selected_index]\n",
    "    def split_table_wrt_value(self, table,value,column):\n",
    "        table1 = table[table[column] <= value]\n",
    "        table2 = table[table[column] > value]\n",
    "        return table1,table2\n",
    "\n",
    "    # returns the value and probability for the leaf which have maximum probability\n",
    "    def get_value_for_leaf(self, table, target_column):\n",
    "        freq = dict()\n",
    "        unique_classes = table[target_column].unique()\n",
    "        for class_name in unique_classes:\n",
    "            freq[class_name] = 0\n",
    "        for index, row in table.iterrows():\n",
    "            freq[row[target_column]] += 1\n",
    "        mx = 0\n",
    "        value = None\n",
    "        sum_of_freq = 0.0\n",
    "        for class_name, freq_of_class in freq.items():\n",
    "            sum_of_freq += freq_of_class\n",
    "            if mx < freq_of_class:\n",
    "                mx = freq_of_class\n",
    "                value = class_name\n",
    "        return value, mx / sum_of_freq\n",
    "    \n",
    "    def build_tree(self, table, height = 0):\n",
    "        # print(\"height =\", height, \"shape =\", table.shape)\n",
    "        if table.shape[1] == 1 or len(table) <= self.THRESHOLD_SIZE or len(table) == 0:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # step1 find the best split\n",
    "        selected_index, column_name, value, best_IG = self.get_best_column(table, self.TARGET_COLUMN, self.IS_CATEGORICAL)\n",
    "        tables = []\n",
    "        decisions = []\n",
    "        condition = None\n",
    "        # categorical value\n",
    "        if value == None:\n",
    "            tables = self.split_dataset_wrt_column(table, column_name)\n",
    "            start_index = 0\n",
    "            for splited_table in tables:\n",
    "                decisions.append(splited_table.iloc[0][column_name])\n",
    "                start_index += len(splited_table)\n",
    "            condition = [None, column_name]\n",
    "        # Continuous Value\n",
    "        else:\n",
    "            table1, table2 = self.split_table_wrt_value(table, value, column_name)\n",
    "            table1 = table1.drop([column_name], axis = 1)\n",
    "            table2 = table2.drop([column_name], axis = 1)\n",
    "            tables.append(table1)\n",
    "            tables.append(table2)\n",
    "            decisions = [None,None]\n",
    "            condition = [value, column_name]\n",
    "        # put some base condition\n",
    "        if best_IG <= self.THRESHOLD_VALUE or len(table) <= self.THRESHOLD_SIZE:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # make tree for each child\n",
    "        children = []\n",
    "        for table in tables:\n",
    "            child = self.build_tree(table, height + 1)\n",
    "            children.append(child)\n",
    "        # return the current node which is already linked to its children so that current node's parent can link current node\n",
    "        return Node(condition, children,decisions)\n",
    "    \n",
    "    def print_tree(self, root, spacing=\"\"):\n",
    "        if isinstance(root, Leaf):\n",
    "            print(spacing, root.leaf_value)\n",
    "            return\n",
    "        print(spacing, root.condition,root.decisions)\n",
    "        for child in root.children:\n",
    "            self.print_tree(child,spacing+\"--> \")\n",
    "    def print_decision_tree(self):\n",
    "        self.print_tree(self.root)\n",
    "    \n",
    "    def find_index(self, arr,x):\n",
    "        for i in range (len(arr)):\n",
    "            if(arr[i] == x):\n",
    "                return i\n",
    "        return -1\n",
    "    def predict_util(self, row,root, starting_index = 0):\n",
    "        #base case\n",
    "        # print(row)\n",
    "        if(isinstance(root,Leaf)):\n",
    "            return root.leaf_value\n",
    "        value_to_check = row.at[starting_index, root.condition[1]]\n",
    "        if root.condition[0] != None:\n",
    "            split_value = root.condition[0]\n",
    "            if value_to_check <= split_value:\n",
    "                child_index = 0\n",
    "            else:\n",
    "                child_index = 1\n",
    "        else:\n",
    "            child_index = self.find_index(root.decisions, value_to_check)\n",
    "        # print(value_to_check, root.decisions)\n",
    "        # print(child_index)\n",
    "        return self.predict_util(row, root.children[child_index], starting_index)\n",
    "\n",
    "        \n",
    "    def predict_for_table(self, table, root):\n",
    "        predictions = []\n",
    "        for index, row in table.iterrows():\n",
    "            row = row.to_frame().T\n",
    "            # print(\"index =\", index)\n",
    "            # print('predictions =', predict(row, root, index))\n",
    "            predictions.append(self.predict_util(row, root, index))\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.predict_for_table(data, self.root)\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data, THRESHOLD_FOR_ACCURACY = 1):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted)\n",
    "            score += (abs(actual - predicted[0]) <= THRESHOLD_FOR_ACCURACY)\n",
    "\n",
    "        print(score / len(testing_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(indexes, x):\n",
    "    indices = []\n",
    "    for i in indexes:\n",
    "        if i == x:\n",
    "            indices.append(True)\n",
    "        else:\n",
    "            indices.append(False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    def __init__(self, dataset, k, num_of_iterations, TARGET_COLUMN = 'new_cases_classes', Tree_Type = \"entropy\"):\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.k = k\n",
    "        self.num_of_iterations = num_of_iterations\n",
    "        self.indexes, self.centroids = kmeans(dataset.drop([TARGET_COLUMN], axis = 1), k, num_of_iterations)\n",
    "        self.dataset = dataset\n",
    "        self.DTs = self.create_trees()\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "\n",
    "    def prepare_data_util(self, x):\n",
    "        dataset1 = self.dataset[get_indices(self.indexes, x)]\n",
    "        dataset1.reset_index(drop=True, inplace=True)\n",
    "        return dataset1\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets = []\n",
    "        for i in range(self.k):\n",
    "            datasets.append(self.prepare_data_util(i))\n",
    "        return datasets\n",
    "\n",
    "    def create_trees(self):\n",
    "        DTs = []\n",
    "        datasets = self.prepare_data()\n",
    "        # print(len(datasets))\n",
    "        for dataset in datasets:\n",
    "            if self.Tree_Type == \"entropy\":\n",
    "                DTs.append(DecisionTree(dataset))\n",
    "            else:\n",
    "                DTs.append(DecisionTree(dataset, Tree_Type = \"CART\"))\n",
    "        return DTs\n",
    "\n",
    "    def calculate_average(self,  all_predictions, centroids):\n",
    "        average_predicted = []\n",
    "        # print(all_predictions)\n",
    "        row, col = len(all_predictions), len(all_predictions[0])\n",
    "        # print(row,col)\n",
    "        for i in range(col):\n",
    "            sum = 0\n",
    "            average_predicted.append(all_predictions[centroids[i]][i])\n",
    "\n",
    "        return(average_predicted)\n",
    "\n",
    "    def predict(self, testing_data):\n",
    "        all_predictions = []\n",
    "        # print(self.DTs)\n",
    "        for DT in self.DTs:\n",
    "            individual_prediction = DT.predict(testing_data)\n",
    "            all_predictions.append(individual_prediction)\n",
    "        # print(all_predictions)\n",
    "        closest_centroids = find_closest_centroids(testing_data.drop([self.TARGET_COLUMN], axis = 1), self.centroids)\n",
    "        final_predictions = self.calculate_average(all_predictions, closest_centroids)\n",
    "        return final_predictions\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data, THRESHOLD_FOR_ACCURACY = 1):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        # THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted[0])\n",
    "            score += (abs(actual - predicted[0]) <= THRESHOLD_FOR_ACCURACY)\n",
    "\n",
    "        print(score / len(testing_data) * 100)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset5.sample(frac = 0.7)\n",
    "testing_data = dataset5.drop(training_data.index)\n",
    "training_data.reset_index(drop=True, inplace=True)\n",
    "testing_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000009?line=0'>1</a>\u001b[0m forest_ID3A \u001b[39m=\u001b[39m Forest(training_data, k \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, num_of_iterations \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 8'\u001b[0m in \u001b[0;36mForest.__init__\u001b[1;34m(self, dataset, k, num_of_iterations, TARGET_COLUMN, Tree_Type)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentroids \u001b[39m=\u001b[39m kmeans(dataset\u001b[39m.\u001b[39mdrop([TARGET_COLUMN], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), k, num_of_iterations)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDTs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_trees()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTARGET_COLUMN \u001b[39m=\u001b[39m TARGET_COLUMN\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 8'\u001b[0m in \u001b[0;36mForest.create_trees\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=26'>27</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTree_Type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=27'>28</a>\u001b[0m         DTs\u001b[39m.\u001b[39mappend(DecisionTree(dataset))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=28'>29</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=29'>30</a>\u001b[0m         DTs\u001b[39m.\u001b[39mappend(DecisionTree(dataset, Tree_Type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCART\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.__init__\u001b[1;34m(self, dataset, THRESHOLD_VALUE, THRESHOLD_SIZE, IS_CATEGORICAL, TARGET_COLUMN, Tree_Type)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTree_Type \u001b[39m=\u001b[39m Tree_Type\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_tree(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset)\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, table, height)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=212'>213</a>\u001b[0m children \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=213'>214</a>\u001b[0m \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=214'>215</a>\u001b[0m     child \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_tree(table, height \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=215'>216</a>\u001b[0m     children\u001b[39m.\u001b[39mappend(child)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=216'>217</a>\u001b[0m \u001b[39m# return the current node which is already linked to its children so that current node's parent can link current node\u001b[39;00m\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, table, height)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_tree\u001b[39m(\u001b[39mself\u001b[39m, table, height \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=183'>184</a>\u001b[0m     \u001b[39m# print(\"height =\", height, \"shape =\", table.shape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=184'>185</a>\u001b[0m     \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(table) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTHRESHOLD_SIZE \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(table) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=185'>186</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Leaf(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_value_for_leaf(table, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTARGET_COLUMN))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=186'>187</a>\u001b[0m     \u001b[39m# step1 find the best split\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=187'>188</a>\u001b[0m     selected_index, column_name, value, best_IG \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_best_column(table, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTARGET_COLUMN, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mIS_CATEGORICAL)\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.get_value_for_leaf\u001b[1;34m(self, table, target_column)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=178'>179</a>\u001b[0m         mx \u001b[39m=\u001b[39m freq_of_class\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=179'>180</a>\u001b[0m         value \u001b[39m=\u001b[39m class_name\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=180'>181</a>\u001b[0m \u001b[39mreturn\u001b[39;00m value, mx \u001b[39m/\u001b[39;49m sum_of_freq\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "forest_ID3A = Forest(training_data, k = 5, num_of_iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000010?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m forest_ID3A\u001b[39m.\u001b[39;49mcalculate_accuracy(testing_data)\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 8'\u001b[0m in \u001b[0;36mForest.calculate_accuracy\u001b[1;34m(self, testing_data, THRESHOLD_FOR_ACCURACY)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_accuracy\u001b[39m(\u001b[39mself\u001b[39m, testing_data, THRESHOLD_FOR_ACCURACY \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=55'>56</a>\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(testing_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=56'>57</a>\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=57'>58</a>\u001b[0m     \u001b[39m# THRESHOLD_FOR_ACCURACY = 1\u001b[39;00m\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 8'\u001b[0m in \u001b[0;36mForest.predict\u001b[1;34m(self, testing_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=45'>46</a>\u001b[0m \u001b[39m# print(self.DTs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m DT \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDTs:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=47'>48</a>\u001b[0m     individual_prediction \u001b[39m=\u001b[39m DT\u001b[39m.\u001b[39;49mpredict(testing_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=48'>49</a>\u001b[0m     all_predictions\u001b[39m.\u001b[39mappend(individual_prediction)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000007?line=49'>50</a>\u001b[0m \u001b[39m# print(all_predictions)\u001b[39;00m\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=265'>266</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=266'>267</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_for_table(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.predict_for_table\u001b[1;34m(self, table, root)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=259'>260</a>\u001b[0m     row \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mto_frame()\u001b[39m.\u001b[39mT\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=260'>261</a>\u001b[0m     \u001b[39m# print(\"index =\", index)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=261'>262</a>\u001b[0m     \u001b[39m# print('predictions =', predict(row, root, index))\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=262'>263</a>\u001b[0m     predictions\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_util(row, root, index))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=263'>264</a>\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 6'\u001b[0m in \u001b[0;36mDecisionTree.predict_util\u001b[1;34m(self, row, root, starting_index)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=250'>251</a>\u001b[0m     child_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_index(root\u001b[39m.\u001b[39mdecisions, value_to_check)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=251'>252</a>\u001b[0m \u001b[39m# print(value_to_check, root.decisions)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=252'>253</a>\u001b[0m \u001b[39m# print(child_index)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000005?line=253'>254</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_util(row, root\u001b[39m.\u001b[39;49mchildren[child_index], starting_index)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions = forest_ID3A.calculate_accuracy(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = forest_ID3A.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Machine learning Lab\\assignment_1\\ML_code_1\\new_k_mean.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000012?line=1'>2</a>\u001b[0m ci \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000012?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000012?line=3'>4</a>\u001b[0m     plt\u001b[39m.\u001b[39mscatter(dataset[\u001b[39m'\u001b[39m\u001b[39mnew_cases_classes\u001b[39m\u001b[39m'\u001b[39m], dataset[\u001b[39m'\u001b[39m\u001b[39mnew_deaths\u001b[39m\u001b[39m'\u001b[39m], c \u001b[39m=\u001b[39m colors[ci])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20learning%20Lab/assignment_1/ML_code_1/new_k_mean.ipynb#ch0000012?line=4'>5</a>\u001b[0m     ci \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "colors = ['r', 'g', 'b', 'y']\n",
    "ci = 0\n",
    "for dataset in datasets:\n",
    "    plt.scatter(dataset['new_cases_classes'], dataset['new_deaths'], c = colors[ci])\n",
    "    ci += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ae141d93f6337e9a20a8395b8018e64e99a1e5c84b295f1c46fe5520871454d"
  },
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
