{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "DISTANCE_TYPE = 'euclidean'\n",
    "DISTANCE_TYPE2 = 'cityblock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset, k, no_of_iterations):\n",
    "    indices = np.random.choice(len(dataset), k, replace = False)\n",
    "    # print(indices)\n",
    "    # choose the rows corresponding to indices which is randomly selected\n",
    "    centroids = dataset.iloc[indices, :]\n",
    "    # the below line finds the distance between centroids and all the datapoints\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    # print(centroids)\n",
    "    # print(distances)\n",
    "    # structure of distance => [[for one point mindistance from each centroid], ...]\n",
    "    # the below line assigns each point with the nearest centroid\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    # the main algo \n",
    "    for iteration in range(no_of_iterations):\n",
    "        # below array will store the centroids\n",
    "        centroids = []\n",
    "        # finding the new centroid for each of the k clusters\n",
    "        for cluster in range(k):\n",
    "            temp_centroid = dataset[points == cluster].mean(axis = 0)\n",
    "            centroids.append(temp_centroid)\n",
    "        \n",
    "        # new centroids\n",
    "        centroids = np.vstack(centroids)\n",
    "\n",
    "        distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "        points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    \n",
    "    return points, centroids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = pd.read_csv(r'processed_covid_data.csv')  # 'processed_covid_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(dataset, centroids):\n",
    "    distances = cdist(dataset, centroids, DISTANCE_TYPE)\n",
    "    points = np.array([np.argmin(dist_from_each_centroid) for dist_from_each_centroid in distances])\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, condition, children, decisions):\n",
    "        self.condition = condition\n",
    "        self.children = children\n",
    "        self.decisions = decisions\n",
    "        \n",
    "class Leaf:\n",
    "    def __init__(self, leaf_value):\n",
    "        self.leaf_value = leaf_value\n",
    "        self.condition = \"THIS IS A LEAF NODE!!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, dataset = None, THRESHOLD_VALUE = 0.0, THRESHOLD_SIZE = 10, IS_CATEGORICAL = [0,0,0,0,0,0,0,0,0,0,0], TARGET_COLUMN = \"new_cases_classes\", Tree_Type = \"entropy\", THRESHOLD_FOR_ACCURACY = 1):\n",
    "        # this cell contains all the constants please beware\n",
    "        self.THRESHOLD_VALUE = THRESHOLD_VALUE\n",
    "        self.THRESHOLD_SIZE = THRESHOLD_SIZE\n",
    "        self.IS_CATEGORICAL = IS_CATEGORICAL\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "        self.dataset = dataset\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.THRESHOLD_FOR_ACCURACY = THRESHOLD_FOR_ACCURACY\n",
    "        self.root = self.build_tree(self.dataset)\n",
    "        \n",
    "    \n",
    "    def set_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    # helper function for getting probability from frequency table\n",
    "    # this function is used in get_entropy()\n",
    "    def get_probability(self, event_info):\n",
    "        SUM = sum(event_info)\n",
    "        for i in range(len(event_info)):\n",
    "                event_info[i] /= SUM\n",
    "        return event_info\n",
    "\n",
    "\n",
    "    # this function gets entropy from frequency table\n",
    "    def get_entropy(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        entropy = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                entropy += p * math.log(1 / p) / math.log(2)\n",
    "        return entropy\n",
    "\n",
    "    # this function gets gini impurity from frequency table\n",
    "    def get_gini_impurity(self, event_info):\n",
    "        probabilities = self.get_probability(event_info)\n",
    "        # print('probabilities', probabilities)\n",
    "        imp = 0.0\n",
    "        for p in probabilities:\n",
    "            if p != 0:\n",
    "                imp += p * (1.0 - p)\n",
    "        return imp\n",
    "   \n",
    "    #this funtion sort table by column\n",
    "    def sort_table_by_column(self, table, col):\n",
    "        return table.sort_values(by = [col]).reset_index(drop=True)\n",
    "   \n",
    "    def split_dataset_wrt_column(self, dataset, column_name):\n",
    "        unique_items = dataset[column_name].unique()\n",
    "        tables = []\n",
    "        for item in unique_items:\n",
    "            tables.append(dataset[dataset[column_name] == item])\n",
    "        return tables\n",
    "    def get_count(self, table, target_column, class_name):\n",
    "        # print(class_name)\n",
    "        # ans =  (table[target_column] == class_name).shape[0]\n",
    "        ans = (table[target_column] == class_name).sum()\n",
    "        # print (ans)\n",
    "        return ans\n",
    "    def get_entropy_from_table(self, table, target_column):\n",
    "        unique_classes = table[target_column].unique()\n",
    "        # print(unique_classes)\n",
    "        counts = []\n",
    "        for class_name in unique_classes:\n",
    "            counts.append(self.get_count(table,target_column, class_name))\n",
    "        # print(\"Count is \", counts)\n",
    "        if self.Tree_Type == \"entropy\":\n",
    "            return self.get_entropy(counts)\n",
    "        else:\n",
    "            return self.get_gini_impurity(counts)    # write get_gini_impurity if you want to change the parameter to gini imp from entropy\n",
    "    # this function returns the information gain of the column \"column\" when the target column is \"target_column\" of the table dataset\n",
    "    # only for categorical column or attribute\n",
    "    def get_information_gain(self, dataset, column, target_column):\n",
    "        tables = []\n",
    "        size_table = []\n",
    "        overall_size = dataset.shape[0]\n",
    "        for table in self.split_dataset_wrt_column(dataset, column):\n",
    "            tables.append(table)\n",
    "            size_table.append(table.shape[0])\n",
    "        entropies = []\n",
    "        for table in tables:\n",
    "            # print(table)\n",
    "            entropies.append(self.get_entropy_from_table(table, target_column))\n",
    "        # print(\"entropies=\", entropies)\n",
    "        # entropies = [get_entropy_from_table(table, target_column) for table in tables]\n",
    "        # print(entropies)\n",
    "        entropy_initial = self.get_entropy_from_table(dataset, target_column)    # entropy without splitting\n",
    "        # print(\"entropy_intial=\",entropy_initial)\n",
    "        entropy = sum([(size / overall_size) * entropyi for size, entropyi in zip(size_table, entropies)])  # entropy after splitting\n",
    "        return (entropy_initial - entropy)\n",
    "\n",
    "    def max_index(self, arr):\n",
    "        index = 0\n",
    "        mx = arr[0]\n",
    "        for i in range(len(arr)):\n",
    "            if mx < arr[i]:\n",
    "                index = i\n",
    "                mx = arr[i]\n",
    "        return index\n",
    "    \n",
    "\n",
    "    def get_value_with_min_entropy_wrt_continuous_column(self, table, column, target_column):\n",
    "        # step 1: sort the table\n",
    "        new_table = self.sort_table_by_column(table, column)\n",
    "        # print(new_table)\n",
    "        # step 2: get various averages\n",
    "        avg_array = []\n",
    "        length_new_table = len(new_table)\n",
    "        for i in range(length_new_table - 1):\n",
    "            avg_array.append((new_table.at[i,column] + new_table.at[i + 1, column]) / 2)\n",
    "        \n",
    "        # print(avg_array)\n",
    "        # step 3: count before and after averages\n",
    "        IGs = []\n",
    "        parentIG = self.get_entropy_from_table(new_table, target_column)\n",
    "        table1 = None\n",
    "        table2 = None\n",
    "        for i in range(length_new_table - 1):\n",
    "            table1 = new_table.iloc[:i + 1,:]\n",
    "            table2 = new_table.iloc[i + 1:, :]\n",
    "            # print('Table 1')\n",
    "            # print(table1)\n",
    "            # print('Table 2')\n",
    "            # print(table2)\n",
    "            E1 = self.get_entropy_from_table(table1, target_column)\n",
    "            E2 = self.get_entropy_from_table(table2, target_column)\n",
    "            # print('E1=',E1, 'E2=', E2)\n",
    "            E = (len(table1) / len(table)) * E1 + (len(table2) / len(table)) * E2\n",
    "            IG = parentIG - E\n",
    "            IGs.append(IG)\n",
    "        # print(IGs)\n",
    "        # if(len(avg_array) == 0):\n",
    "        #     IGs = [0]\n",
    "        #     avg_array = [1]\n",
    "        index = self.max_index(IGs)\n",
    "        \n",
    "        return avg_array[index], IGs[index]  # split wrt value, IG according to that value\n",
    "        # step 4: calculate the entropy wrt each average\n",
    "        # step 5: determine the best split with most information gain\n",
    "        #This function returns the best column for the split\n",
    "    def get_best_column (self, table, target_column, is_categorical):\n",
    "        values = []\n",
    "        # is_categorical is an array which is true if the data is categorical and false if continuous\n",
    "        IGs = []\n",
    "        for index,column in enumerate(table):\n",
    "            if(column == target_column):\n",
    "                break\n",
    "            # print(column,index)\n",
    "            if(is_categorical[index] == 1):\n",
    "                values.append(None)\n",
    "                IGs.append(self.get_information_gain(table, column, target_column))\n",
    "            else:\n",
    "                #value is the value at which the splitting occurs in the column and IG is the corresponding Info gain\n",
    "                value, IG = self.get_value_with_min_entropy_wrt_continuous_column(table,column,target_column)\n",
    "                values.append(value)\n",
    "                # print(value)\n",
    "                IGs.append(IG)\n",
    "        selected_index = self.max_index(IGs)\n",
    "        if(is_categorical[selected_index]):\n",
    "            return selected_index, table.columns[selected_index], None, IGs[selected_index]\n",
    "        else:\n",
    "            return selected_index, table.columns[selected_index],values[selected_index], IGs[selected_index]\n",
    "    def split_table_wrt_value(self, table,value,column):\n",
    "        table1 = table[table[column] <= value]\n",
    "        table2 = table[table[column] > value]\n",
    "        return table1,table2\n",
    "\n",
    "    # returns the value and probability for the leaf which have maximum probability\n",
    "    def get_value_for_leaf(self, table, target_column):\n",
    "        freq = dict()\n",
    "        unique_classes = table[target_column].unique()\n",
    "        for class_name in unique_classes:\n",
    "            freq[class_name] = 0\n",
    "        for index, row in table.iterrows():\n",
    "            freq[row[target_column]] += 1\n",
    "        mx = 0\n",
    "        value = None\n",
    "        sum_of_freq = 0.0\n",
    "        for class_name, freq_of_class in freq.items():\n",
    "            sum_of_freq += freq_of_class\n",
    "            if mx < freq_of_class:\n",
    "                mx = freq_of_class\n",
    "                value = class_name\n",
    "        return value, mx / sum_of_freq\n",
    "    \n",
    "    def build_tree(self, table, height = 0):\n",
    "        # print(\"height =\", height, \"shape =\", table.shape)\n",
    "        if table.shape[1] == 1 or len(table) <= self.THRESHOLD_SIZE:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # step1 find the best split\n",
    "        selected_index, column_name, value, best_IG = self.get_best_column(table, self.TARGET_COLUMN, self.IS_CATEGORICAL)\n",
    "        tables = []\n",
    "        decisions = []\n",
    "        condition = None\n",
    "        # categorical value\n",
    "        if value == None:\n",
    "            tables = self.split_dataset_wrt_column(table, column_name)\n",
    "            start_index = 0\n",
    "            for splited_table in tables:\n",
    "                decisions.append(splited_table.iloc[0][column_name])\n",
    "                start_index += len(splited_table)\n",
    "            condition = [None, column_name]\n",
    "        # Continuous Value\n",
    "        else:\n",
    "            table1, table2 = self.split_table_wrt_value(table, value, column_name)\n",
    "            table1 = table1.drop([column_name], axis = 1)\n",
    "            table2 = table2.drop([column_name], axis = 1)\n",
    "            if len(table1) > 0: \n",
    "                tables.append(table1)\n",
    "            if len(table2) > 0:\n",
    "                tables.append(table2)\n",
    "            decisions = [None,None]\n",
    "            condition = [value, column_name]\n",
    "        # put some base condition\n",
    "        if best_IG <= self.THRESHOLD_VALUE or len(table) <= self.THRESHOLD_SIZE:\n",
    "            return Leaf(self.get_value_for_leaf(table, self.TARGET_COLUMN))\n",
    "        # make tree for each child\n",
    "        children = []\n",
    "        for table in tables:\n",
    "            if len(table) > 0:\n",
    "                child = self.build_tree(table, height + 1)\n",
    "                children.append(child)\n",
    "        # return the current node which is already linked to its children so that current node's parent can link current node\n",
    "        return Node(condition, children,decisions)\n",
    "    \n",
    "    def print_tree(self, root, spacing=\"\"):\n",
    "        if isinstance(root, Leaf):\n",
    "            print(spacing, root.leaf_value)\n",
    "            return\n",
    "        print(spacing, root.condition,root.decisions)\n",
    "        for child in root.children:\n",
    "            self.print_tree(child,spacing+\"--> \")\n",
    "    def print_decision_tree(self):\n",
    "        self.print_tree(self.root)\n",
    "    \n",
    "    def find_index(self, arr,x):\n",
    "        for i in range (len(arr)):\n",
    "            if(arr[i] == x):\n",
    "                return i\n",
    "    def predict_util(self, row,root, starting_index = 0):\n",
    "        #base case\n",
    "        # print(row)\n",
    "        if(isinstance(root,Leaf)):\n",
    "            return root.leaf_value\n",
    "        value_to_check = row.at[starting_index, root.condition[1]]\n",
    "        if root.condition[0] != None:\n",
    "            split_value = root.condition[0]\n",
    "            if value_to_check <= split_value:\n",
    "                child_index = 0\n",
    "            else:\n",
    "                child_index = 1\n",
    "        else:\n",
    "            child_index = self.find_index(root.decisions, value_to_check)\n",
    "        # print(value_to_check, root.decisions)\n",
    "        # print(len(root.children))\n",
    "        if (child_index >= len(root.children)):\n",
    "            return -1\n",
    "        return self.predict_util(row, root.children[child_index], starting_index)\n",
    "\n",
    "        \n",
    "    def predict_for_table(self, table, root):\n",
    "        predictions = []\n",
    "        for index, row in table.iterrows():\n",
    "            row = row.to_frame().T\n",
    "            # print(\"index =\", index)\n",
    "            # print('predictions =', predict(row, root, index))\n",
    "            predictions.append(self.predict_util(row, root, index))\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.predict_for_table(data, self.root)\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        DIFF = 0.0\n",
    "        THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted)\n",
    "            if self.Tree_Type == 'entropy':\n",
    "                DIFF += abs(actual - predicted[0])\n",
    "                score += (abs(actual - predicted[0]) <= self.THRESHOLD_FOR_ACCURACY)\n",
    "            else:\n",
    "                DIFF += abs(actual - predicted)\n",
    "                score += (abs(actual - predicted) <= self.THRESHOLD_FOR_ACCURACY)\n",
    "\n",
    "        print(score / len(testing_data) * 100)\n",
    "        return predictions, DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(indexes, x):\n",
    "    indices = []\n",
    "    for i in indexes:\n",
    "        if i == x:\n",
    "            indices.append(True)\n",
    "        else:\n",
    "            indices.append(False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    def __init__(self, dataset, k, num_of_iterations, TARGET_COLUMN = 'new_cases_classes', Tree_Type = \"entropy\"):\n",
    "        self.Tree_Type = Tree_Type\n",
    "        self.k = k\n",
    "        self.num_of_iterations = num_of_iterations\n",
    "        self.indexes, self.centroids = kmeans(dataset.drop([TARGET_COLUMN], axis = 1), k, num_of_iterations)\n",
    "        self.dataset = dataset\n",
    "        self.DTs = self.create_trees()\n",
    "        self.TARGET_COLUMN = TARGET_COLUMN\n",
    "\n",
    "    def prepare_data_util(self, x):\n",
    "        dataset1 = self.dataset[get_indices(self.indexes, x)]\n",
    "        dataset1.reset_index(drop=True, inplace=True)\n",
    "        return dataset1\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets = []\n",
    "        for i in range(self.k):\n",
    "            datasets.append(self.prepare_data_util(i))\n",
    "        return datasets\n",
    "\n",
    "    def create_trees(self):\n",
    "        DTs = []\n",
    "        datasets = self.prepare_data()\n",
    "        # print(len(datasets))\n",
    "        for dataset in datasets:\n",
    "            if self.Tree_Type == \"entropy\":\n",
    "                DTs.append(DecisionTree(dataset))\n",
    "            else:\n",
    "                DTs.append(DecisionTree(dataset, Tree_Type = \"CART\"))\n",
    "        return DTs\n",
    "\n",
    "    def calculate_average(self,  all_predictions, centroids):\n",
    "        average_predicted = []\n",
    "        # print(all_predictions)\n",
    "        row, col = len(all_predictions), len(all_predictions[0])\n",
    "        # print(row,col)\n",
    "        for i in range(col):\n",
    "            sum = 0\n",
    "            average_predicted.append(all_predictions[centroids[i]][i]) # centroids[i] = closest DT for ith datapoint, second [i] means we need to get the prediction for ith datapoint\n",
    "            # average_predicted = all_predictions[centroids[i]]\n",
    "        return(average_predicted)\n",
    "\n",
    "    def predict(self, testing_data):\n",
    "        all_predictions = []\n",
    "        # print(self.DTs)\n",
    "        for DT in self.DTs:\n",
    "            individual_prediction = DT.predict(testing_data)\n",
    "            all_predictions.append(individual_prediction)\n",
    "        # print(all_predictions)\n",
    "        closest_centroids = find_closest_centroids(testing_data.drop([self.TARGET_COLUMN], axis = 1), self.centroids)\n",
    "        final_predictions = self.calculate_average(all_predictions, closest_centroids)\n",
    "        return final_predictions\n",
    "    \n",
    "    def calculate_accuracy(self, testing_data, THRESHOLD_FOR_ACCURACY = 1):\n",
    "        predictions = self.predict(testing_data)\n",
    "        score = 0\n",
    "        # THRESHOLD_FOR_ACCURACY = 1\n",
    "        for i in range(len(testing_data)):\n",
    "            actual = testing_data.at[i, self.TARGET_COLUMN]\n",
    "            predicted = predictions[i]\n",
    "            # print(actual, predicted[0])\n",
    "            score += (abs(actual - predicted[0]) <= THRESHOLD_FOR_ACCURACY)\n",
    "        accuracy = (score / len(testing_data) * 100)\n",
    "        print(score / len(testing_data) * 100)\n",
    "        return predictions,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset5.sample(frac = 0.7)\n",
    "testing_data = dataset5.drop(training_data.index)\n",
    "training_data.reset_index(drop=True, inplace=True)\n",
    "testing_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_ID3A = Forest(training_data, k = 5, num_of_iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.53731343283582\n"
     ]
    }
   ],
   "source": [
    "predictions = forest_ID3A.calculate_accuracy(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_CART = Forest(training_data, k = 5, num_of_iterations = 100, Tree_Type = 'CART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.28358208955223\n"
     ]
    }
   ],
   "source": [
    "predictions = forest_CART.calculate_accuracy(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "# for k in range(3,5):\n",
    "#     print(f\"Running for {k}\")\n",
    "#     forest_ID3A = Forest(training_data, k, num_of_iterations = 100)\n",
    "#     predictions,accuracy = forest_ID3A.calculate_accuracy(testing_data)\n",
    "#     accuracies.append(accuracy)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(accuracies)\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Accuracy in %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = forest_ID3A.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'g', 'b', 'y', 'c']\n",
    "ci = 0\n",
    "for dataset in datasets:\n",
    "    plt.scatter(dataset['new_cases_classes'], dataset['new_deaths'], c = colors[ci])\n",
    "    ci += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ae141d93f6337e9a20a8395b8018e64e99a1e5c84b295f1c46fe5520871454d"
  },
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
